import { Message } from "@langchain/langgraph-sdk";

export function extractStringFromMessageContent(message: Message): string {
  return typeof message.content === "string"
    ? message.content
    : Array.isArray(message.content)
      ? message.content
          .filter(
            (c: unknown) =>
              (typeof c === "object" &&
                c !== null &&
                "type" in c &&
                (c as { type: string }).type === "text") ||
              typeof c === "string",
          )
          .map((c: unknown) =>
            typeof c === "string"
              ? c
              : typeof c === "object" && c !== null && "text" in c
                ? (c as { text?: string }).text || ""
                : "",
          )
          .join("")
      : "";
}

export function isPreparingToCallTaskTool(messages: Message[]): boolean {
  const lastMessage = messages[messages.length - 1];
  return (
    (lastMessage.type === "ai" &&
      lastMessage.tool_calls?.some(
        (call: { name?: string }) => call.name === "task",
      )) ||
    false
  );
}

export function justCalledTaskTool(messages: Message[]): boolean {
  const lastAiMessage = messages.findLast((message) => message.type === "ai");
  if (!lastAiMessage) return false;
  const toolMessagesAfterLastAiMessage = messages.slice(
    messages.indexOf(lastAiMessage) + 1,
  );
  const taskToolCallsCompleted = toolMessagesAfterLastAiMessage.some(
    (message) => message.type === "tool" && message.name === "task",
  );
  return (
    (lastAiMessage.tool_calls?.some(
      (call: { name?: string }) => call.name === "task",
    ) &&
      taskToolCallsCompleted) ||
    false
  );
}

export function formatMessageForLLM(message: Message): string {
  let role: string;
  if (message.type === "human") {
    role = "Human";
  } else if (message.type === "ai") {
    role = "Assistant";
  } else if (message.type === "tool") {
    role = `Tool Result`;
  } else {
    role = message.type || "Unknown";
  }

  const timestamp = message.id ? ` (${message.id.slice(0, 8)})` : "";
  
  let contentText = "";
  
  // Extract content text
  if (typeof message.content === "string") {
    contentText = message.content;
  } else if (Array.isArray(message.content)) {
    const textParts: string[] = [];
    
    message.content.forEach((part: any) => {
      if (typeof part === "string") {
        textParts.push(part);
      } else if (part && typeof part === "object" && part.type === "text") {
        textParts.push(part.text || "");
      }
      // Ignore other types like tool_use in content - we handle tool calls separately
    });
    
    contentText = textParts.join("\n\n").trim();
  }
  
  // For tool messages, include additional tool metadata
  if (message.type === "tool") {
    const toolName = (message as any).name || "unknown_tool";
    const toolCallId = (message as any).tool_call_id || "";
    role = `Tool Result [${toolName}]`;
    if (toolCallId) {
      role += ` (call_id: ${toolCallId.slice(0, 8)})`;
    }
  }
  
  // Handle tool calls from .tool_calls property (for AI messages)
  const toolCallsText: string[] = [];
  if (message.tool_calls && Array.isArray(message.tool_calls) && message.tool_calls.length > 0) {
    message.tool_calls.forEach((call: any) => {
      const toolName = call.name || "unknown_tool";
      const toolArgs = call.args ? JSON.stringify(call.args, null, 2) : "{}";
      toolCallsText.push(`[Tool Call: ${toolName}]\nArguments: ${toolArgs}`);
    });
  }
  
  // Combine content and tool calls
  const parts: string[] = [];
  if (contentText) {
    parts.push(contentText);
  }
  if (toolCallsText.length > 0) {
    parts.push(...toolCallsText);
  }
  
  if (parts.length === 0) {
    return `${role}${timestamp}: [Empty message]`;
  }
  
  if (parts.length === 1) {
    return `${role}${timestamp}: ${parts[0]}`;
  }
  
  return `${role}${timestamp}:\n${parts.join("\n\n")}`;
}

export function formatConversationForLLM(messages: Message[]): string {
  const formattedMessages = messages.map(formatMessageForLLM);
  return formattedMessages.join("\n\n---\n\n");
}

export function prepareOptimizerMessage(feedback: string): string {
  return `<feedback>
${feedback}
</feedback>

You have access to the current configuration in config.yaml and the conversation history in conversation.txt. Use the above feedback to update the config.yaml file based on the conversation context.
`;
}
